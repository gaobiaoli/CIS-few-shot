{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaobiaoli/miniconda3/envs/openmmlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ClipAdapter import DirDataset\n",
    "from ClipAdapter import ClipAdapter\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from torchvision import transforms\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 74.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Zero-shot CLIP's val accuracy: 43.52. ****\n",
      "\n",
      "[0.46778351 0.36151603]\n",
      "[0.62371134 0.23091248]\n",
      "[0.53460972 0.28181818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#zero shot full dataset\n",
    "root='dataset/rebar_tying'\n",
    "class_dir_map={\n",
    "    \"a photo of a worker squatting or bending to tie steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work or taking a break\":\"3\",\n",
    "}\n",
    "\n",
    "dataset_val=DirDataset(root=root,class_dir_map=class_dir_map,transform=preprocess)\n",
    "dataloader_val=DataLoader(dataset=dataset_val,batch_size=16,num_workers=4,shuffle=True)\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    text = clip.tokenize(list(class_dir_map.keys()))\n",
    "    text=text.to(device)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i,(imgs,label,_) in enumerate(tqdm(dataloader_val)):\n",
    "        imgs= imgs.to(device)\n",
    "        image_features = clip_model.encode_image(imgs)\n",
    "        image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        logits_per_image = 100. * image_features_norm @ text_features.t()\n",
    "\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu()\n",
    "        pred_label=(probs.argmax(dim=1))\n",
    "\n",
    "        all_targets.extend(label.cpu().numpy())\n",
    "        all_predictions.extend(pred_label.cpu().numpy())\n",
    "\n",
    "accuracy = metrics.accuracy_score(all_targets, all_predictions)\n",
    "precision = metrics.precision_score(all_targets, all_predictions, average=None)\n",
    "recall = metrics.recall_score(all_targets, all_predictions,average=None)\n",
    "f1 = metrics.f1_score(all_targets, all_predictions,average=None)\n",
    "print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy*100))\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.02it/s]\n",
      "100%|██████████| 68/68 [00:04<00:00, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best HP, beta: 1.00, alpha: 5.09; accuracy: 82.89\n",
      "\n",
      "**** Zero-shot CLIP's val accuracy: 53.54. ****\n",
      "\n",
      "[0.68944099 0.50863931]\n",
      "[0.19611307 0.90403071]\n",
      "[0.30536451 0.65100207]\n",
      "\n",
      "**** Few-shot CLIP's val accuracy: 82.89. ****\n",
      "\n",
      "[0.86964981 0.79232112]\n",
      "[0.78975265 0.87140115]\n",
      "[0.82777778 0.82998172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "# clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "class_dir_map={\n",
    "    \"a photo of a worker tying steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work\":\"3\",\n",
    "}\n",
    "if True:\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_tranform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    root='dataset/rebar_tying'\n",
    "    seed=1000\n",
    "    dataset_shot=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed)\n",
    "\n",
    "    dataloader_shot=DataLoader(dataset=dataset_shot,batch_size=16,num_workers=4,shuffle=False)\n",
    "    clip_adapter=ClipAdapter(model=clip_model,dataloader=dataloader_shot,classnames=dataset_shot.classnames,augment_epoch=10,alpha=10,beta=1,device=device)\n",
    "    dataset_test=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed,reverse=True)\n",
    "    dataloader_test=DataLoader(dataset=dataset_test,batch_size=16,num_workers=4,shuffle=True)\n",
    "    clip_adapter.pre_load_features(dataloader_test)\n",
    "    clip_adapter.search_hp(beta_search=False)\n",
    "    all_predictions, all_targets,(accuracy0,precision,recall,f1)=clip_adapter.eval(adapt=False)\n",
    "    print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy0*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)\n",
    "    all_predictions, all_targets,(accuracy1,precision,recall,f1)=clip_adapter.eval(adapt=True)\n",
    "    print(\"\\n**** Few-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy1*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)\n",
    "\n",
    "    with open('result/rebar_few_shot.txt', 'w') as file:\n",
    "        for item in dataset_shot.imgs_list:\n",
    "            file.write(str(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
