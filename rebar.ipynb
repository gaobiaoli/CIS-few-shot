{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from ClipAdapter import DirDataset\n",
    "from ClipAdapter import ClipAdapter\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from torchvision import transforms\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:01<00:00, 44.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Zero-shot CLIP's val accuracy: 45.04. ****\n",
      "\n",
      "[0.48135593 0.33333333]\n",
      "[0.73195876 0.1452514 ]\n",
      "[0.5807771  0.20233463]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#zero shot full dataset\n",
    "root=r'/CV/gaobiaoli/dataset/rebar_tying'\n",
    "class_dir_map={\n",
    "    \"a photo of a worker squatting or bending to tie steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work or taking a break\":\"3\",\n",
    "}\n",
    "\n",
    "dataset_val=DirDataset(root=root,class_dir_map=class_dir_map,transform=preprocess)\n",
    "dataloader_val=DataLoader(dataset=dataset_val,batch_size=16,num_workers=4,shuffle=True)\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    text = clip.tokenize(list(class_dir_map.keys()))\n",
    "    text=text.to(device)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i,(imgs,label,_) in enumerate(tqdm(dataloader_val)):\n",
    "        imgs= imgs.to(device)\n",
    "        image_features = clip_model.encode_image(imgs)\n",
    "        image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        logits_per_image = 100. * image_features_norm @ text_features.t()\n",
    "\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu()\n",
    "        pred_label=(probs.argmax(dim=1))\n",
    "\n",
    "        all_targets.extend(label.cpu().numpy())\n",
    "        all_predictions.extend(pred_label.cpu().numpy())\n",
    "\n",
    "accuracy = metrics.accuracy_score(all_targets, all_predictions)\n",
    "precision = metrics.precision_score(all_targets, all_predictions, average=None)\n",
    "recall = metrics.recall_score(all_targets, all_predictions,average=None)\n",
    "f1 = metrics.f1_score(all_targets, all_predictions,average=None)\n",
    "print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy*100))\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val.imgs_list)\n",
    "len(os.listdir(\"/CV/gaobiaoli/dataset/rebar_tying/3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n",
      "100%|██████████| 68/68 [00:08<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best HP, beta: 1.00, alpha: 7.58; accuracy: 80.86\n",
      "\n",
      "**** Zero-shot CLIP's val accuracy: 37.44. ****\n",
      "\n",
      "[0.321875   0.39634941]\n",
      "[0.1819788  0.58349328]\n",
      "[0.23250564 0.47204969]\n",
      "\n",
      "**** Few-shot CLIP's val accuracy: 80.86. ****\n",
      "\n",
      "[0.78870968 0.83511777]\n",
      "[0.8639576  0.74856046]\n",
      "[0.82462057 0.78947368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "# clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "class_dir_map={\n",
    "    \"a photo of a worker tying steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work\":\"3\",\n",
    "}\n",
    "if True:\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_tranform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    root=r'/CV/gaobiaoli/dataset/rebar_tying'\n",
    "    seed=1000\n",
    "    dataset_shot=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed)\n",
    "    dataloader_shot=DataLoader(dataset=dataset_shot,batch_size=16,num_workers=4,shuffle=False)\n",
    "    clip_adapter=ClipAdapter(model=clip_model,dataloader=dataloader_shot,classnames=dataset_shot.classnames,augment_epoch=10,alpha=10,beta=1,device=device)\n",
    "    dataset_test=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed,reverse=True)\n",
    "    dataloader_test=DataLoader(dataset=dataset_test,batch_size=16,num_workers=4,shuffle=True)\n",
    "    clip_adapter.pre_load_features(dataloader_test)\n",
    "    clip_adapter.search_hp(beta_search=False)\n",
    "    all_predictions, all_targets,(accuracy0,precision,recall,f1)=clip_adapter.eval(adapt=False)\n",
    "    print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy0*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)\n",
    "    all_predictions, all_targets,(accuracy1,precision,recall,f1)=clip_adapter.eval(adapt=True)\n",
    "    print(\"\\n**** Few-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy1*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
