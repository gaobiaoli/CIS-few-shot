{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaobiaoli/miniconda3/envs/openmmlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ClipAdapter import DirDataset\n",
    "from ClipAdapter import ClipAdapter\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from torchvision import transforms\n",
    "from utils.utils import inin_random\n",
    "model_name=\"ViT-L/14\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:08<00:00,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Zero-shot CLIP's val accuracy: 48.44. ****\n",
      "\n",
      "[0.5030525  0.43333333]\n",
      "[0.70790378 0.24208566]\n",
      "[0.58815132 0.31063321]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#zero shot full dataset\n",
    "clip_model, preprocess = clip.load(model_name, device=device)\n",
    "root='dataset/rebar_tying'\n",
    "class_dir_map={\n",
    "    \"a photo of a worker squatting or bending to tie steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work or taking a break\":\"3\",\n",
    "}\n",
    "\n",
    "dataset_val=DirDataset(root=root,class_dir_map=class_dir_map,transform=preprocess)\n",
    "dataloader_val=DataLoader(dataset=dataset_val,batch_size=16,num_workers=4,shuffle=True)\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    text = clip.tokenize(list(class_dir_map.keys()))\n",
    "    text=text.to(device)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i,(imgs,label,_) in enumerate(tqdm(dataloader_val)):\n",
    "        imgs= imgs.to(device)\n",
    "        image_features = clip_model.encode_image(imgs)\n",
    "        image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        logits_per_image = 100. * image_features_norm @ text_features.t()\n",
    "\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu()\n",
    "        pred_label=(probs.argmax(dim=1))\n",
    "\n",
    "        all_targets.extend(label.cpu().numpy())\n",
    "        all_predictions.extend(pred_label.cpu().numpy())\n",
    "\n",
    "accuracy = metrics.accuracy_score(all_targets, all_predictions)\n",
    "precision = metrics.precision_score(all_targets, all_predictions, average=None)\n",
    "recall = metrics.recall_score(all_targets, all_predictions,average=None)\n",
    "f1 = metrics.f1_score(all_targets, all_predictions,average=None)\n",
    "print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy*100))\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "import torch\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "model=resnet50(num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    total = 0.\n",
    "    for idx, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_corrects += torch.sum(preds.eq(labels))\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total += labels.size(0)\n",
    "    total_loss = total_loss / total\n",
    "    acc = 100 * total_corrects / total\n",
    "    print(\"轮次:%4d|训练集损失:%.5f|训练集准确率:%6.2f%%\" % (epoch + 1, total_loss, acc))\n",
    "    return total_loss, acc\n",
    " \n",
    " \n",
    "def test_model(model, test_loader, loss_fn, optimizer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(tqdm(test_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total += labels.size(0)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_corrects += torch.sum(preds.eq(labels))\n",
    " \n",
    "        loss = total_loss / total\n",
    "        accuracy = 100 * total_corrects / total\n",
    "        print(\"轮次:%4d|训练集损失:%.5f|训练集准确率:%6.2f%%\" % (epoch + 1, loss, accuracy))\n",
    "        return loss, accuracy\n",
    " \n",
    " \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "epoches=10\n",
    "for epoch in range(0, epoches):\n",
    "    loss1, acc1 = train_model(model, dataloader_train, loss_fn, optimizer, epoch)\n",
    "    loss2, acc2 = test_model(model, dataloader_val, loss_fn, optimizer, epoch)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.65it/s]\n",
      "100%|██████████| 68/68 [00:06<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best HP, beta: 1.00, alpha: 5.09; accuracy: 82.89\n",
      "\n",
      "**** Zero-shot CLIP's val accuracy: 53.54. ****\n",
      "\n",
      "[0.68944099 0.50863931]\n",
      "[0.19611307 0.90403071]\n",
      "[0.30536451 0.65100207]\n",
      "\n",
      "**** Few-shot CLIP's val accuracy: 82.89. ****\n",
      "\n",
      "[0.86964981 0.79232112]\n",
      "[0.78975265 0.87140115]\n",
      "[0.82777778 0.82998172]\n"
     ]
    }
   ],
   "source": [
    "clip_model, preprocess = clip.load(model_name, device=device)\n",
    "class_dir_map={\n",
    "    \"a photo of a worker tying steel bars\":\"12\",\n",
    "    \"a photo of a worker doing non-rebar work\":\"3\",\n",
    "}\n",
    "if True:\n",
    "    seed = 1\n",
    "    inin_random(seed)\n",
    "    train_tranform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    # root='dataset/rebar_tying'\n",
    "    root=r'/CV/gaobiaoli/dataset/rebar_tying'\n",
    "    seed=1000\n",
    "    dataset_shot=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed)\n",
    "    dataloader_shot=DataLoader(dataset=dataset_shot,batch_size=16,num_workers=4,shuffle=False)\n",
    "    clip_adapter=ClipAdapter(model=clip_model,dataloader=dataloader_shot,classnames=dataset_shot.classnames,augment_epoch=10,alpha=10,beta=1,device=device)\n",
    "    dataset_test=DirDataset(root=root,class_dir_map=class_dir_map,transform=train_tranform,few_shot=16,random_seed=seed,reverse=True)\n",
    "    dataloader_test=DataLoader(dataset=dataset_test,batch_size=16,num_workers=4,shuffle=True)\n",
    "    clip_adapter.pre_load_features(dataloader_test)\n",
    "    clip_adapter.search_hp(beta_search=False)\n",
    "    all_predictions, all_targets,(accuracy0,precision,recall,f1)=clip_adapter.eval(adapt=False)\n",
    "    print(\"\\n**** Zero-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy0*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)\n",
    "    all_predictions, all_targets,(accuracy1,precision,recall,f1)=clip_adapter.eval(adapt=True)\n",
    "    print(\"\\n**** Few-shot CLIP's val accuracy: {:.2f}. ****\\n\".format(accuracy1*100))\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
